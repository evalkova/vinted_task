{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin by extracting our data into Pandas dataframes. I have created a simple python module just for that task. \n",
    "\n",
    "All dataframes will have duplicate rows deleted and empty strings will be substituted with NaN values. This is a very basic transformation that can be done for almost any data. Further transformations will depend on what we see in our dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract, config\n",
    "import pandas as pd\n",
    "\n",
    "# Specifying confgiguration path and initiating an Extractor\n",
    "datasources_config_path = './config/data_source_config.json'\n",
    "extractor = extract.Extractor(datasources_config_path)\n",
    "\n",
    "# Extracting dataframes\n",
    "product_invoices_df = extractor.load_json_files_to_df('product_invoices')\n",
    "product_package_types_df = extractor.load_json_files_to_df('product_package_types')\n",
    "product_shipments_df = extractor.load_json_files_to_df('product_shipments')\n",
    "provider_invoices_df = extractor.load_json_files_to_df('provider_invoices')\n",
    "provider_prices_df = extractor.load_json_files_to_df('provider_prices')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking and Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check our dataframes one by one and fix anything funky that might be going on with them.\n",
    "\n",
    "I will focus on missing data and potential primary keys that are not unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transform\n",
    "transformer = transform.Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of transactions where we have collected the shipment fee from the buyers\n",
    "product_invoices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic checks\n",
    "transformer.check_if_unique_id(product_invoices_df, 'transaction_id')\n",
    "transformer.check_missing_values(product_invoices_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red background returned here looks scary but that is just my logs doing what they are supposed to - we can see that the transaction_id is unique and there are no missing values. Moving on to the next dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of package types we have on our platform that a seller can select.\n",
    "product_package_types_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description data in the product_package_types_df does not look uniform. Some values include both dimensions and weight of the packages. Dimensions are not referred to in any of the other tables. We will transform this data so that a weight value is saved in a separate column. This way we can make the most of it - for example, we can later compare whether the shipping information matches the information provided by the sellers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_package_types_df['weight_reported'] = product_package_types_df['description'].apply(transformer.cleanup_description)\n",
    "product_package_types_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that looks much more usable! I will do the rest of the checks now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic checks\n",
    "transformer.check_if_unique_id(product_package_types_df, 'id')\n",
    "transformer.check_missing_values(product_package_types_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there were missing description values, but the new weight_reported transformed them - you can't see it here, but it set them to the number 0. This still doesn't seem completely right, but we won't drop these values as their IDs migth still be referred to in other tables.\n",
    "\n",
    "Let's move on to the next dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shipping labels that the shipping provider is charging us for.\n",
    "provider_invoices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic checks\n",
    "transformer.check_if_unique_id(provider_invoices_df, 'tracking_code')\n",
    "transformer.check_missing_values(provider_invoices_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the values with duplicated tracking_code\n",
    "ids = provider_invoices_df[\"tracking_code\"]\n",
    "provider_invoices_df_duplicated_tracking = provider_invoices_df[ids.isin(ids[ids.duplicated()])]\n",
    "provider_invoices_df_duplicated_tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the tracking code is not unique and there are 4 rows affected. To me, this seems like a data quality issue. In order to join this dataframe with other dataframes, we can either drop the above data or keep the most recent record. I am choosing to drop it in the interest of faster analysis but in a real life scenario, I would flag such an issue and keep hold of that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping duplicated values and checking whether tracking_code is unique now - this can be extracted in a separate function\n",
    "duplicates = provider_invoices_df_duplicated_tracking.tracking_code.unique()\n",
    "provider_invoices_df = provider_invoices_df[~provider_invoices_df.tracking_code.isin(duplicates)]\n",
    "\n",
    "transformer.check_if_unique_id(provider_invoices_df, 'tracking_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another transformation we can do here is to convert the measured weight from gr to kg. I see other tables have weight saved in kg, so such transformation will make future comparisons easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming provider_invoices_df weight to be in kg\n",
    "transformed_provider_invoices_df = provider_invoices_df.copy();\n",
    "transformed_provider_invoices_df[\"weight_measured\"] = provider_invoices_df[\"weight_measured\"]/1000\n",
    "transformed_provider_invoices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to the next dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The list of shipments that we see in our data\n",
    "product_shipments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic checks\n",
    "transformer.check_if_unique_id(product_shipments_df, 'transaction_id')\n",
    "transformer.check_if_unique_id(product_shipments_df, 'tracking_code')\n",
    "transformer.check_missing_values(product_shipments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the values with duplicated tracking_code\n",
    "ids = product_shipments_df[\"tracking_code\"]\n",
    "product_shipments_df_duplicated_tracking = product_shipments_df[ids.isin(ids[ids.duplicated()])]\n",
    "product_shipments_df_duplicated_tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same repeated tracking_ids can be found in the product_shipments_df data and these codes are clearaly associated with different shipments. We will get rid of them for now in order to be able to join the product_shipments and provider_invoices data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicated values and checking whether tracking_code is unique now - this can be extracted in a separate function\n",
    "duplicates = product_shipments_df_duplicated_tracking.tracking_code.unique()\n",
    "product_shipments_df = product_shipments_df[~product_shipments_df.tracking_code.isin(duplicates)]\n",
    "transformer.check_if_unique_id(product_shipments_df, 'tracking_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder whether the repeated tracking code will be found in the product_invoices data - we can find that based on the unique transaction IDs we saw for the tracking codes in the  product_shipments data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_invoices_df\n",
    "\n",
    "duplicates = [8638716, 271780177, 7789938, 260812237]\n",
    "\n",
    "product_invoices_df.loc[product_invoices_df['transaction_id'].isin(duplicates)]\n",
    "# product_invoices_df = product_invoices_df.set_index(['transaction_id'])\n",
    "# print(product_invoices_df.loc[7789938])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The repeated tracking code is in the product_invoices data. We can drop it but when we join this table with the others, it won't matter as the values will be dropped anyway. \n",
    "\n",
    "There is one more dataframe to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prices that we are being charged by the provider based on weight, route of the shipment.\n",
    "provider_prices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like reference data from the provider, which we can use to double-check whether the provider charges us fairly. We could remove the kg in actual_package_size, which would make comparisons easier. I will leave this task for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier - let's join product_shipments_df, product_invoices_df, and provider_invoices_df! This will allow us to have one main dataset, which will make explorations and comparisons easier. We know we can treat transaction_id as a unique key in product_invoices_df and product_shipments_df. Now we also know that tracking_code is unique and since it exists in both product_shipments_df and provider_invoices_df, we can easily perform the join.\n",
    "\n",
    "I will first rename the columns that exist in more than one table so we don't get confused when the join happnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_product_shipments_df = product_shipments_df.rename(columns={'from_country':'from_country_product', 'to_country':'to_country_product'})\n",
    "renamed_product_invoices_df = product_invoices_df.rename(columns={'amount':'amount_product'})\n",
    "renamed_provider_invoices_df = transformed_provider_invoices_df.rename(columns={'from_country':'from_country_provider', 'to_country':'to_country_provider', \n",
    "                                     'amount':'amount_provider'})\n",
    "\n",
    "product_invoices_shipments_df = pd.merge(renamed_product_shipments_df, renamed_product_invoices_df, on='transaction_id')\n",
    "product_provider_invoices_shipments_df = pd.merge(product_invoices_shipments_df, renamed_provider_invoices_df, \n",
    "                                                  on='tracking_code')\n",
    "\n",
    "full_df = pd.merge(product_package_types_df[['weight_reported', 'id']],product_provider_invoices_shipments_df,\n",
    "                                                  left_on='id', right_on='package_type_id', how='right')\n",
    "\n",
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am noticing that we have dropped some records when joining product_shipments_df, product_invoices_df, and provider_invoices_df. It would be useful to check whether there are provider invoices that do not correspond to any product invoices, but for the sake of the current task, I am assuming any such discrepancies might be because of incomplete data and I will focus on more exciting explorations.\n",
    "\n",
    "The next thing we can do is to check whether the duplicated columns in the tables we just joined match and if not - we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether duplicated columns from joined tables match. This can be more elegant.\n",
    "isFromCountrySame = full_df['from_country_product'].equals(full_df['from_country_provider'])\n",
    "if isFromCountrySame:\n",
    "     full_df.drop('from_country_provider', axis=1, inplace=True)\n",
    "    \n",
    "isToCountrySame = full_df['to_country_product'].equals(full_df['to_country_provider'])\n",
    "if isToCountrySame:\n",
    "     full_df.drop('to_country_provider', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing amount columns\n",
    "isAmtSame = full_df['amount_product'].equals(full_df['amount_provider'])\n",
    "print(isAmtSame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is interesting - we can see that the amount in the product invoices differs from the amount in the provider invoices. We need to explore further. Something that might be useful is a new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with a basic description of the numeric values in our new main table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the amount in the product invoices is slightly higher on average than the amount in the provider invoices. The lower standart deviation of the amount in the product invoices suggest that data is a bit less varied and tends to be closer to the mean compared to the amount in the provider invoices. But when is the discrepancy observed? Bring on the graphs - we are about to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution graph for amount_provider\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "full_df.round({'amount_product': 2, 'amount_provider': 2})\n",
    "plotAmtProvider = sns.countplot(x='amount_provider',data=full_df.round(2))\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of amount in provider invoices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution graph for amount_product\n",
    "plotAmtProduct = sns.countplot(x=\"amount_product\",data=full_df.round(2))\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of amount in product invoices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the majoritiy of amounts for both product and provider invoices are one of three values. Additionally, as expected, the distributions differ and the product invoices have more invoices with higher amounts.\n",
    "\n",
    "Now let's see how the amounts in both types of invoices have changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the type of some columns to aid further exploration\n",
    "full_df[[\"shipping_label_created\", \"user_invoice_date\"]] = full_df[[\"shipping_label_created\", \"user_invoice_date\"]].apply(pd.to_datetime)\n",
    "full_df[\"shipping_label_created_year_month\"] = full_df[\"shipping_label_created\"].dt.strftime('%Y-%m')\n",
    "full_df['weight_reported'] = full_df['weight_reported'].astype(float)\n",
    "\n",
    "# Creating a dataframe with mean values per month\n",
    "mean_full_df = full_df.groupby('shipping_label_created_year_month').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot - amount\n",
    "sns.lineplot(x=\"shipping_label_created_year_month\", y=\"amount_product\", data=mean_full_df)\n",
    "sns.lineplot(x=\"shipping_label_created_year_month\", y=\"amount_provider\", data=mean_full_df)\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Amount')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(['Product', 'Provider'])\n",
    "plt.title('Mean amount in product invoices versus mean amount in provider invoices shown per month')\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, ignore the INFO logs - I would configure multiple loggers so that the graphs are not affected by INFO logging but I am short on time.\n",
    "\n",
    "We can see that the discrepancy between the product and provider amount has reduced with time and the most recent data has almost identical mean amounts.\n",
    "\n",
    "Let's see the same relationship in regard to the mean weight customers selected versus the mean weight recorded by the provider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot - weight\n",
    "sns.lineplot(x=\"shipping_label_created_year_month\", y=\"weight_reported\", data=mean_full_df)\n",
    "sns.lineplot(x=\"shipping_label_created_year_month\", y=\"weight_measured\", data=mean_full_df)\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Weight in kg')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(['Weight reported by customers', 'Weight recorded in provider invoices'])\n",
    "plt.title('Mean weight measured by provider and mean weight reported by customers per month')\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, it seems like the weight selected by users is higher than the actual reported weight. This discrepancy makes sense given the weight measured by providers is the actual weight and the weight provided by sellers is an estimation of the maximum weight their package could be. \n",
    "\n",
    "The mean reported and measured weight are getting closer with time, which indicates that seller's estimations are getting better - perhaps due to more options for package sizes. Average weights of packages also seems to be increasing - as Vinted was getting more popular, perhaps it was getting more common for sellers to ship bigger items or several items in the same package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's step away from mean values and explore counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with unique values per month\n",
    "unique_full_df = full_df.groupby('shipping_label_created_year_month').nunique()\n",
    "unique_full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am noticing that the unique number of buyers and sellers is consistently similar and it is also increasing over time. Besides user increase, another growth measure is the number of transactions over time, which I will plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot - transactions\n",
    "sns.lineplot(x=\"shipping_label_created_year_month\", y=\"transaction_id\", data=unique_full_df)\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Number of transactions')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Number of transactions per month')\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of transactions has been increasing steadily since 2017. The beginning of this growth seem to correlate with a drop in the mean shipping price charged by provider. I am assuming a better deal was negotiated in June 2017 due to the increase in shippings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am curious whether the discrepency between the product and provider invoices depend on the type of shipping - local vs international."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data so we can see split by local and international shipping\n",
    "full_df['is_trans_country'] = (full_df['from_country_product']==full_df['to_country_product']).astype(int)\n",
    "full_df\n",
    "\n",
    "mean_full_df_is_trans = full_df.groupby(['is_trans_country'])\n",
    "mean_full_df_is_trans_false = mean_full_df_is_trans.get_group(0)\n",
    "mean_full_df_is_trans_true = mean_full_df_is_trans.get_group(1)\n",
    "\n",
    "mean_full_df_is_trans_false_mean = mean_full_df_is_trans_false.groupby(['shipping_label_created_year_month']).mean()\n",
    "mean_full_df_is_trans_true_mean = mean_full_df_is_trans_true.groupby(['shipping_label_created_year_month']).mean()\n",
    "\n",
    "# Line plot - amount for international shippings\n",
    "sns.lineplot(x=\"shipping_label_created_year_month\", y=\"amount_product\", data=mean_full_df_is_trans_true_mean)\n",
    "sns.lineplot(x=\"shipping_label_created_year_month\", y=\"amount_provider\", data=mean_full_df_is_trans_true_mean)\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Amount')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(['Trans Country - Product', 'Trans Country - Provider'])\n",
    "plt.title('Mean amount in product invoices versus mean amount in provider invoices for international shippings shown per month')\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product invoices versus mean amount in provider invoices for international shippings looks very similar to the graph we had for all shippings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot - amount for local shippings\n",
    "sns.lineplot(x=\"shipping_label_created_year_month\", y=\"amount_product\", data=mean_full_df_is_trans_false_mean)\n",
    "sns.lineplot(x=\"shipping_label_created_year_month\", y=\"amount_provider\", data=mean_full_df_is_trans_false_mean)\n",
    "\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Amount')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(['In Country - Product', 'In Country - Provider'])\n",
    "plt.title('Mean amount in product invoices versus mean amount in provider invoices for local shippings shown per month')\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data for local shippings covers only a small period but is still consistent with the trends we found so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the findings\n",
    "\n",
    "There is a discrepancy between the amounts in the provider’s invoices and the product’s invoices but this discrepancy seems to be reducing with time.\n",
    "\n",
    "There is also a difference in the weights measured by providers and the weights reported by customers.\n",
    "\n",
    "The data shows Vinted's obvious growth rate. The beginning of the growth is associated with a decrease in the mean amount in the provider’s invoices. With an increase in shippings, Vinted could have negotiated lower prices with the shipping companies as it became a bigger customer.\n",
    "\n",
    "### Limitations and further explorations\n",
    "\n",
    "A lot of my graphs depended on means, which only gives a glimpse at the data. I also didn't have the time to use the provider_prices data, which could have helped explore whether providers are overcharging Vinted. \n",
    "\n",
    "I could have integrated Tableau, which would have made the exploration faster.\n",
    "\n",
    "Churn is another important metric, which could have been explored with this dataset - I could have looked at customer retention rate assuming that the buyer/seller IDs are unique to a buyer/seller and do not change.\n",
    "\n",
    "This type of data seems great for future predictive modelling with linear regression for example.\n",
    "\n",
    "It would be useful to get information regarding different changes Vinted has made, such as new deals with providers/new providers, new package sizing systems.  \n",
    "\n",
    "### Design\n",
    "\n",
    "The design of the current solution could be improved. My Jupyter notebook is used as the orchestrator of the application but that would not be the case in a production environment.\n",
    "\n",
    "I would imagine the application to have batch processing, which would allow for monitoring of some key indicators daily and/or weekly. Reference data can be stored  separately so we don’t load and clean it all the time. As we load files, we need to keep track of the files that have already been loaded. Transformations and visualisations should be abstracted, which I could have done if I had more time. Caching can be used for faster processing. The loading process is not part of this task but the data could be stored in a warehouse. There could also be a separation of different independent pipelines, which would enable multiprocessing. Usage of cloud solutions would provide an additional performance boost. Clear uniform naming conventions and following specific standards across all data pipelines would also benefit a growing system.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
